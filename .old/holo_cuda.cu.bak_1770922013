/*
 * Holographic Image Transmission Protocol — CUDA Implementation (Refactored)
 * =========================================================================
 *
 * Supports:
 * - Stateful Solvers: CGLS (L2), ISTA (L1)
 * - Real-time iteration stepping
 * - Host-side Histogram Projection (Pinned Memory Optimized)
 *
 * Build:
 *   nvcc -O3 -shared -Xcompiler -fPIC -o libholo.so holo_cuda.cu -lcurand
 * -lcublas -lm -DBUILD_LIB
 */

#include <algorithm>
#include <cublas_v2.h>
#include <cuda_runtime.h>
#include <curand.h>
#include <math.h>
#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <vector>

#define BATCH_SIZE 512

/* Fatal versions */
#define CHECK_CUDA(call)                                                       \
  do {                                                                         \
    cudaError_t e = (call);                                                    \
    if (e != cudaSuccess) {                                                    \
      fprintf(stderr, "CUDA error %s:%d: %s\n", __FILE__, __LINE__,            \
              cudaGetErrorString(e));                                          \
      exit(1);                                                                 \
    }                                                                          \
  } while (0)
#define CHECK_CURAND(call)                                                     \
  do {                                                                         \
    curandStatus_t e = (call);                                                 \
    if (e != CURAND_STATUS_SUCCESS) {                                          \
      fprintf(stderr, "cuRAND error %s:%d: %d\n", __FILE__, __LINE__, e);      \
      exit(1);                                                                 \
    }                                                                          \
  } while (0)
#define CHECK_CUBLAS(call)                                                     \
  do {                                                                         \
    cublasStatus_t e = (call);                                                 \
    if (e != CUBLAS_STATUS_SUCCESS) {                                          \
      fprintf(stderr, "cuBLAS error %s:%d: %d\n", __FILE__, __LINE__, e);      \
      exit(1);                                                                 \
    }                                                                          \
  } while (0)

#define SAFE_CUDA(call)                                                        \
  do {                                                                         \
    cudaError_t e = (call);                                                    \
    if (e != cudaSuccess) {                                                    \
      fprintf(stderr, "CUDA error %s:%d: %s\n", __FILE__, __LINE__,            \
              cudaGetErrorString(e));                                          \
      goto fail;                                                               \
    }                                                                          \
  } while (0)
#define SAFE_CURAND(call)                                                      \
  do {                                                                         \
    curandStatus_t e = (call);                                                 \
    if (e != CURAND_STATUS_SUCCESS) {                                          \
      fprintf(stderr, "cuRAND error %s:%d: %d\n", __FILE__, __LINE__, e);      \
      goto fail;                                                               \
    }                                                                          \
  } while (0)
#define SAFE_CUBLAS(call)                                                      \
  do {                                                                         \
    cublasStatus_t e = (call);                                                 \
    if (e != CUBLAS_STATUS_SUCCESS) {                                          \
      fprintf(stderr, "cuBLAS error %s:%d: %d\n", __FILE__, __LINE__, e);      \
      goto fail;                                                               \
    }                                                                          \
  } while (0)

/* Kernels */

/*
 * MurmurHash3 Mix - Fast, high-quality non-cryptographic hash
 * We use this to generate mask values deterministically without memory access.
 */
__device__ __forceinline__ uint32_t murmur_mix(uint32_t k) {
  k ^= k >> 16;
  k *= 0x85ebca6b;
  k ^= k >> 13;
  k *= 0xc2b2ae35;
  k ^= k >> 16;
  return k;
}

/*
 * get_mask_val: Returns +1.0f or -1.0f
 * deterministically based on seed, pixel index, and measurement index.
 */
__device__ __forceinline__ float get_mask_val(uint64_t seed, int pix_idx,
                                              int meas_idx) {
  uint32_t h = (uint32_t)(seed & 0xFFFFFFFF); // Mix low bits of seed
  // Combine indices. We just need *some* unique mapping that is uniform.
  // Using a robust mix ensures good randomness.
  uint32_t k = (uint32_t)pix_idx ^ ((uint32_t)meas_idx * 0x9e3779b9);
  // Add high bits of seed
  k ^= (uint32_t)(seed >> 32);

  uint32_t r = murmur_mix(k ^ h);

  // Bit 0 determines sign.
  return (r & 1) ? 1.0f : -1.0f;
}

__global__ void float_to_mask(float *data, int n) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < n) {
    data[idx] = (data[idx] > 0.5f) ? 1.0f : -1.0f;
  }
}

/* Soft Thresholding: x = sign(x) * max(|x| - lambda, 0) */
__global__ void soft_threshold_kernel(float *x, float lambda, int n) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < n) {
    float val = x[idx];
    float abs_val = fabsf(val);
    if (abs_val <= lambda) {
      x[idx] = 0.0f;
    } else {
      x[idx] = (val > 0.0f) ? (val - lambda) : (val + lambda);
    }
  }
}

/* Common State Structures */
struct TxState {
  int width, height, npix;
  uint64_t seed;
  uint64_t generated_count; /* Track total measurements for fast gen */
  float *d_image;
  float *d_masks;
  float *d_meas;
  curandGenerator_t curand_gen;
  cublasHandle_t cublas;
};

struct RxState {
  int width, height, npix;
  uint64_t seed;
  float *d_accum; /* Accumulator for direct accumulation */
  float *d_masks;
  float *d_meas;
  curandGenerator_t curand_gen;
  cublasHandle_t cublas;
};

/*
 * Solver State
 * Holds everything needed to step through the solution
 */
struct PixelRank {
  float val;
  int index;
};

/* Sorting helper */
static bool comparePixels(const PixelRank &a, const PixelRank &b) {
  return a.val < b.val;
}

struct SolverState {
  int type; // 1=CGLS (L2), 2=ISTA (L1)
  int width, height, npix;
  int n_meas;
  uint64_t seed;

  cublasHandle_t cublas;
  curandGenerator_t curand_gen;

  /* Common Buffers */
  float *d_x;         /* Solution */
  float *d_b;         /* Measurements */
  float *d_masks;     /* Temp for mask generation */
  float *d_temp_meas; /* Temp for batch operations */

  /* CGLS Specific */
  float *d_r;  /* Residual */
  float *d_s;  /* Direction */
  float *d_p;  /* Conjugate Direction */
  float *d_q;  /* A * p */
  float gamma; /* ||s||^2 */

  /* ISTA Specific */
  float *d_grad; /* Gradient */
  float t_step;  /* Step size */
  float lambda;  /* Sparsity penalty */

  /* Host Buffers for Sorting (Pinned Memory) */
  float *h_x_pinned;
  float *h_target_pinned;
  PixelRank *h_ranked; /* malloc'd plain array */
};

/* Helper: A * x (Forward) */
static void ops_Ax(SolverState *s, const float *d_x, float *d_ax) {
  float alpha = 1.0f, beta = 0.0f;
  int batch_size = BATCH_SIZE;
  int processed = 0;

  CHECK_CURAND(curandSetPseudoRandomGeneratorSeed(s->curand_gen, s->seed));
  CHECK_CURAND(curandSetGeneratorOffset(s->curand_gen, 0));

  while (processed < s->n_meas) {
    int chunk = s->n_meas - processed;
    if (chunk > batch_size)
      chunk = batch_size;

    CHECK_CURAND(curandGenerateUniform(s->curand_gen, s->d_masks,
                                       (size_t)chunk * s->npix));
    int threads = 256;
    int blocks = ((size_t)chunk * s->npix + threads - 1) / threads;
    float_to_mask<<<blocks, threads>>>(s->d_masks, chunk * s->npix);

    CHECK_CUBLAS(cublasSgemv(s->cublas, CUBLAS_OP_T, s->npix, chunk, &alpha,
                             s->d_masks, s->npix, d_x, 1, &beta, s->d_temp_meas,
                             1));

    CHECK_CUDA(cudaMemcpy(d_ax + processed, s->d_temp_meas,
                          chunk * sizeof(float), cudaMemcpyDeviceToDevice));
    processed += chunk;
  }
}

/* Helper: A^T * y (Backward) */
static void ops_ATy(SolverState *s, const float *d_y, float *d_aty) {
  float alpha = 1.0f, beta = 1.0f;
  int batch_size = BATCH_SIZE;
  int processed = 0;

  CHECK_CUDA(cudaMemset(d_aty, 0, s->npix * sizeof(float)));

  CHECK_CURAND(curandSetPseudoRandomGeneratorSeed(s->curand_gen, s->seed));
  CHECK_CURAND(curandSetGeneratorOffset(s->curand_gen, 0));

  while (processed < s->n_meas) {
    int chunk = s->n_meas - processed;
    if (chunk > batch_size)
      chunk = batch_size;

    CHECK_CUDA(cudaMemcpy(s->d_temp_meas, d_y + processed,
                          chunk * sizeof(float), cudaMemcpyDeviceToDevice));

    CHECK_CURAND(curandGenerateUniform(s->curand_gen, s->d_masks,
                                       (size_t)chunk * s->npix));
    int threads = 256;
    int blocks = ((size_t)chunk * s->npix + threads - 1) / threads;
    float_to_mask<<<blocks, threads>>>(s->d_masks, chunk * s->npix);

    CHECK_CUBLAS(cublasSgemv(s->cublas, CUBLAS_OP_N, s->npix, chunk, &alpha,
                             s->d_masks, s->npix, s->d_temp_meas, 1, &beta,
                             d_aty, 1));
    processed += chunk;
  }
}

static void axpby(cublasHandle_t handle, int n, float alpha, const float *x,
                  float beta, float *y) {
  if (beta != 1.0f)
    cublasSscal(handle, n, &beta, y, 1);
  if (alpha != 0.0f)
    cublasSaxpy(handle, n, &alpha, x, 1, y, 1);
}

/*
 * kernel_tx_fast: Generate b = A * x (Forward)
 * Same logic as kernel_Ax_fast
 */
__global__ void kernel_tx_fast(const float *__restrict__ image,
                               float *__restrict__ meas, int n_meas, int npix,
                               uint64_t seed, int meas_offset) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < n_meas) {
    float sum = 0.0f;
    int global_meas_idx = meas_offset + idx;
    for (int j = 0; j < npix; j++) {
      float val = image[j];
      float mask = get_mask_val(seed, j, global_meas_idx);
      sum += val * mask;
    }
    meas[idx] = sum;
  }
}

extern "C" {

TxState *tx_create(uint64_t seed, int width, int height, const float *h_image) {
  TxState *s = new TxState;
  s->seed = seed;
  s->width = width;
  s->height = height;
  s->npix = width * height;
  s->generated_count = 0; // Init counter
  s->d_image = NULL;
  s->d_masks = NULL;
  s->d_meas = NULL;

  SAFE_CUBLAS(cublasCreate(&s->cublas));
  SAFE_CURAND(curandCreateGenerator(&s->curand_gen, CURAND_RNG_PSEUDO_DEFAULT));
  SAFE_CURAND(curandSetPseudoRandomGeneratorSeed(s->curand_gen, seed));

  SAFE_CUDA(cudaMalloc(&s->d_image, s->npix * sizeof(float)));
  SAFE_CUDA(cudaMemcpy(s->d_image, h_image, s->npix * sizeof(float),
                       cudaMemcpyHostToDevice));
  SAFE_CUDA(cudaMalloc(&s->d_masks, BATCH_SIZE * s->npix * sizeof(float)));
  SAFE_CUDA(cudaMalloc(&s->d_meas, BATCH_SIZE * sizeof(float)));
  return s;
fail:
  delete s;
  return NULL;
}

void tx_destroy(TxState *s) {
  if (!s)
    return;
  cudaFree(s->d_image);
  cudaFree(s->d_masks);
  cudaFree(s->d_meas);
  curandDestroyGenerator(s->curand_gen);
  cublasDestroy(s->cublas);
  delete s;
}

void tx_generate(TxState *s, float *h_output, int count) {
  int processed = 0;
  float alpha = 1.0f, beta = 0.0f;
  while (processed < count) {
    int chunk = count - processed;
    if (chunk > BATCH_SIZE)
      chunk = BATCH_SIZE;
    CHECK_CURAND(curandGenerateUniform(s->curand_gen, s->d_masks,
                                       (size_t)chunk * s->npix));
    int threads = 256;
    int blocks = ((size_t)chunk * s->npix + threads - 1) / threads;
    float_to_mask<<<blocks, threads>>>(s->d_masks, chunk * s->npix);
    CHECK_CUBLAS(cublasSgemv(s->cublas, CUBLAS_OP_T, s->npix, chunk, &alpha,
                             s->d_masks, s->npix, s->d_image, 1, &beta,
                             s->d_meas, 1));
    CHECK_CUDA(cudaMemcpy(h_output + processed, s->d_meas,
                          chunk * sizeof(float), cudaMemcpyDeviceToHost));
    processed += chunk;
  }
  CHECK_CUDA(cudaDeviceSynchronize());
}

/*
 * Fast TX Generation (matches Fast Solver)
 * Note: Does not use d_masks or curand.
 * Directly computes b = A * x using on-the-fly masks.
 */
void tx_generate_fast(TxState *s, float *h_output, int count) {
  int processed = 0;
  // Use persistent counter as base offset
  int base_offset = (int)s->generated_count;

  while (processed < count) {
    int chunk = count - processed;
    if (chunk > BATCH_SIZE)
      chunk = BATCH_SIZE;

    int threads = 256;
    int blocks = (chunk + threads - 1) / threads;

    // Kernel uses (meas_offset + idx) to generate masks
    // meas_offset = base + processed
    kernel_tx_fast<<<blocks, threads>>>(s->d_image, s->d_meas, chunk, s->npix,
                                        s->seed, base_offset + processed);

    CHECK_CUDA(cudaMemcpy(h_output + processed, s->d_meas,
                          chunk * sizeof(float), cudaMemcpyDeviceToHost));

    processed += chunk;
  }
  CHECK_CUDA(cudaDeviceSynchronize());

  // Update global counter
  s->generated_count += count;
}

/* Rx State - for Accumulation mode */
RxState *rx_create(uint64_t seed, int width, int height) {

  RxState *s = new RxState;
  s->seed = seed;
  s->width = width;
  s->height = height;
  s->npix = width * height;
  s->d_accum = NULL;
  s->d_masks = NULL;
  s->d_meas = NULL;
  SAFE_CUBLAS(cublasCreate(&s->cublas));
  SAFE_CURAND(curandCreateGenerator(&s->curand_gen, CURAND_RNG_PSEUDO_DEFAULT));
  SAFE_CURAND(curandSetPseudoRandomGeneratorSeed(s->curand_gen, seed));
  SAFE_CUDA(cudaMalloc(&s->d_accum, s->npix * sizeof(float)));
  SAFE_CUDA(cudaMemset(s->d_accum, 0, s->npix * sizeof(float)));
  SAFE_CUDA(cudaMalloc(&s->d_masks, BATCH_SIZE * s->npix * sizeof(float)));
  SAFE_CUDA(cudaMalloc(&s->d_meas, BATCH_SIZE * sizeof(float)));
  return s;
fail:
  delete s;
  return NULL;
}
void rx_destroy(RxState *s) {
  if (!s)
    return;
  cudaFree(s->d_accum);
  cudaFree(s->d_masks);
  cudaFree(s->d_meas);
  curandDestroyGenerator(s->curand_gen);
  cublasDestroy(s->cublas);
  delete s;
}
void rx_accumulate(RxState *s, const float *h_measurements, int count) {
  int processed = 0;
  float alpha = 1.0f, beta = 1.0f;
  while (processed < count) {
    int chunk = count - processed;
    if (chunk > BATCH_SIZE)
      chunk = BATCH_SIZE;
    CHECK_CUDA(cudaMemcpy(s->d_meas, h_measurements + processed,
                          chunk * sizeof(float), cudaMemcpyHostToDevice));
    CHECK_CURAND(curandGenerateUniform(s->curand_gen, s->d_masks,
                                       (size_t)chunk * s->npix));
    int threads = 256;
    int blocks = ((size_t)chunk * s->npix + threads - 1) / threads;
    float_to_mask<<<blocks, threads>>>(s->d_masks, chunk * s->npix);
    CHECK_CUBLAS(cublasSgemv(s->cublas, CUBLAS_OP_N, s->npix, chunk, &alpha,
                             s->d_masks, s->npix, s->d_meas, 1, &beta,
                             s->d_accum, 1));
    processed += chunk;
  }
  CHECK_CUDA(cudaDeviceSynchronize());
}
void rx_get_image(RxState *s, uint8_t *h_output, int total_measurements) {
  if (total_measurements <= 0)
    return;
  float *h_accum = (float *)malloc(s->npix * sizeof(float));
  CHECK_CUDA(cudaMemcpy(h_accum, s->d_accum, s->npix * sizeof(float),
                        cudaMemcpyDeviceToHost));
  for (int i = 0; i < s->npix; i++) {
    float v = h_accum[i] / (float)total_measurements;
    if (v < 0)
      v = 0;
    if (v > 255)
      v = 255;
    h_output[i] = (uint8_t)(v + 0.5f);
  }
  free(h_accum);
}

/*
 */
__global__ void kernel_Ax_fast(const float *__restrict__ x,
                               float *__restrict__ b, int n_meas, int npix,
                               uint64_t seed) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < n_meas) {
    float sum = 0.0f;
    // This loop is long (npix), but accesses x linearly (good for cache)
    // and generates masks on fly (good for bandwidth).
    // Since x fits in L2 for small images, this is super fast.
    for (int j = 0; j < npix; j++) {
      float val = x[j];
      // Only multiply if x is non-zero (sparsity could help if we knew it, but
      // here dense) But branching might hurt. Let's just do it.
      float mask = get_mask_val(seed, j, idx);
      sum += val * mask;
    }
    b[idx] = sum;
  }
}

/*
 * kernel_ATy_fast: x = A^T * y
 * Parallelize over pixels (N threads).
 * Each thread computes one element of x: dot(col_j, y).
 */
__global__ void kernel_ATy_fast(const float *__restrict__ y,
                                float *__restrict__ x, int n_meas, int npix,
                                uint64_t seed) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < npix) {
    float sum = 0.0f;
    // Loop over measurements
    for (int i = 0; i < n_meas; i++) {
      float val = y[i];
      float mask = get_mask_val(seed, idx, i); // Same mask as Ax!
      sum += val * mask;
    }
    x[idx] = sum;
  }
}

/*
 * Helper Wrappers for Fast Ops
 */
static void ops_Ax_fast(SolverState *s, const float *d_x, float *d_ax) {
  int threads = 256;
  int blocks = (s->n_meas + threads - 1) / threads;
  kernel_Ax_fast<<<blocks, threads>>>(d_x, d_ax, s->n_meas, s->npix, s->seed);
}

static void ops_ATy_fast(SolverState *s, const float *d_y, float *d_aty) {
  int threads = 256;
  int blocks = (s->npix + threads - 1) / threads;
  kernel_ATy_fast<<<blocks, threads>>>(d_y, d_aty, s->n_meas, s->npix, s->seed);
}

// ----------------------------------------------------------------------
// Helper: Common Initialization
// ----------------------------------------------------------------------
void solver_init_common(SolverState *s) {
  SAFE_CUDA(cudaHostAlloc((void **)&s->h_x_pinned, s->npix * sizeof(float),
                          cudaHostAllocDefault));
  SAFE_CUDA(cudaHostAlloc((void **)&s->h_target_pinned, s->npix * sizeof(float),
                          cudaHostAllocDefault));
  s->h_ranked = (PixelRank *)malloc(s->npix * sizeof(PixelRank));
  return;
fail:
  fprintf(stderr, "Host alloc failed\n");
}

// ----------------------------------------------------------------------
// Original Solver Creation (Stateful)
// ----------------------------------------------------------------------

SolverState *solver_create_ls(uint64_t seed, int width, int height,
                              const float *h_meas, int n_meas) {
  SolverState *s = new SolverState;
  s->type = 1; // Standard CGLS
  s->seed = seed;
  s->width = width;
  s->height = height;
  s->npix = width * height;
  s->n_meas = n_meas;

  SAFE_CUBLAS(cublasCreate(&s->cublas));
  SAFE_CURAND(curandCreateGenerator(&s->curand_gen, CURAND_RNG_PSEUDO_DEFAULT));

  SAFE_CUDA(cudaMalloc(&s->d_x, s->npix * sizeof(float)));
  SAFE_CUDA(cudaMemset(s->d_x, 0, s->npix * sizeof(float)));

  SAFE_CUDA(cudaMalloc(&s->d_b, n_meas * sizeof(float)));
  SAFE_CUDA(cudaMemcpy(s->d_b, h_meas, n_meas * sizeof(float),
                       cudaMemcpyHostToDevice));

  // CGLS Buffers
  SAFE_CUDA(cudaMalloc(&s->d_r, n_meas * sizeof(float)));
  SAFE_CUDA(cudaMemcpy(s->d_r, s->d_b, n_meas * sizeof(float),
                       cudaMemcpyDeviceToDevice)); // r0 = b

  SAFE_CUDA(cudaMalloc(&s->d_masks, BATCH_SIZE * s->npix * sizeof(float)));
  SAFE_CUDA(cudaMalloc(&s->d_temp_meas, BATCH_SIZE * sizeof(float)));

  SAFE_CUDA(cudaMalloc(&s->d_s, s->npix * sizeof(float)));
  // s0 = A^T r0 (Slow/Standard)
  ops_ATy(s, s->d_r, s->d_s);

  SAFE_CUDA(cudaMalloc(&s->d_p, s->npix * sizeof(float)));
  SAFE_CUDA(cudaMemcpy(s->d_p, s->d_s, s->npix * sizeof(float),
                       cudaMemcpyDeviceToDevice)); // p0 = s0

  SAFE_CUDA(cudaMalloc(&s->d_q, n_meas * sizeof(float)));
  s->d_grad = NULL; // Not used

  cublasSdot(s->cublas, s->npix, s->d_s, 1, s->d_s, 1, &s->gamma);

  solver_init_common(s);
  return s;
fail:
  delete s;
  return NULL;
}

SolverState *solver_create_l1(uint64_t seed, int width, int height,
                              const float *h_meas, int n_meas, float lambda) {
  SolverState *s = new SolverState;
  s->type = 2; // ISTA
  s->seed = seed;
  s->width = width;
  s->height = height;
  s->npix = width * height;
  s->n_meas = n_meas;
  s->lambda = lambda;
  s->t_step = 1.0f / ((float)n_meas > 100 ? (float)n_meas : 100.0f);

  SAFE_CUBLAS(cublasCreate(&s->cublas));
  SAFE_CURAND(curandCreateGenerator(&s->curand_gen, CURAND_RNG_PSEUDO_DEFAULT));

  SAFE_CUDA(cudaMalloc(&s->d_x, s->npix * sizeof(float)));
  SAFE_CUDA(cudaMemset(s->d_x, 0, s->npix * sizeof(float)));

  SAFE_CUDA(cudaMalloc(&s->d_b, n_meas * sizeof(float)));
  SAFE_CUDA(cudaMemcpy(s->d_b, h_meas, n_meas * sizeof(float),
                       cudaMemcpyHostToDevice));

  SAFE_CUDA(cudaMalloc(&s->d_grad, s->npix * sizeof(float)));

  // Aux
  SAFE_CUDA(cudaMalloc(&s->d_masks, BATCH_SIZE * s->npix * sizeof(float)));
  SAFE_CUDA(cudaMalloc(&s->d_temp_meas, BATCH_SIZE * sizeof(float)));

  SAFE_CUDA(cudaMalloc(&s->d_r, n_meas * sizeof(float)));
  s->d_s = NULL;
  s->d_p = NULL;
  s->d_q = NULL;

  solver_init_common(s);
  return s;

fail:
  delete s;
  return NULL;
}

SolverState *solver_create_ls_fast(uint64_t seed, int width, int height,
                                   const float *h_meas, int n_meas) {
  SolverState *s = new SolverState;
  s->type = 10; // FAST CGLS
  s->seed = seed;
  s->width = width;
  s->height = height;
  s->npix = width * height;
  s->n_meas = n_meas;
  s->curand_gen = NULL; // Initialize to NULL

  SAFE_CUBLAS(cublasCreate(&s->cublas));
  // No curand needed!

  SAFE_CUDA(cudaMalloc(&s->d_x, s->npix * sizeof(float)));
  SAFE_CUDA(cudaMemset(s->d_x, 0, s->npix * sizeof(float)));

  SAFE_CUDA(cudaMalloc(&s->d_b, n_meas * sizeof(float)));
  SAFE_CUDA(cudaMemcpy(s->d_b, h_meas, n_meas * sizeof(float),
                       cudaMemcpyHostToDevice));

  // CGLS Buffers
  SAFE_CUDA(cudaMalloc(&s->d_r, n_meas * sizeof(float)));
  SAFE_CUDA(cudaMemcpy(s->d_r, s->d_b, n_meas * sizeof(float),
                       cudaMemcpyDeviceToDevice)); // r0 = b

  SAFE_CUDA(cudaMalloc(&s->d_s, s->npix * sizeof(float)));
  // s0 = A^T r0 (Fast)
  ops_ATy_fast(s, s->d_r, s->d_s);

  SAFE_CUDA(cudaMalloc(&s->d_p, s->npix * sizeof(float)));
  SAFE_CUDA(cudaMemcpy(s->d_p, s->d_s, s->npix * sizeof(float),
                       cudaMemcpyDeviceToDevice)); // p0 = s0

  SAFE_CUDA(cudaMalloc(&s->d_q, n_meas * sizeof(float)));

  // No auxiliary temp buffers needed for masks!
  s->d_masks = NULL;
  s->d_temp_meas = NULL;
  s->d_grad = NULL;

  cublasSdot(s->cublas, s->npix, s->d_s, 1, s->d_s, 1, &s->gamma);

  solver_init_common(s);
  return s;
fail:
  delete s;
  return NULL;
}

SolverState *solver_create_l1_fast(uint64_t seed, int width, int height,
                                   const float *h_meas, int n_meas,
                                   float lambda) {
  SolverState *s = new SolverState;
  s->type = 20; // FAST FISTA
  s->seed = seed;
  s->width = width;
  s->height = height;
  s->npix = width * height;
  s->n_meas = n_meas;
  s->lambda = lambda;
  s->curand_gen = NULL; // Initialize to NULL

  // Estimate Lipschitz constant L.
  // For A with entries {-1, 1}, max eigenvalue of A^T A is roughly n_meas *
  // npix (upper bound?) Actually, spectral radius is approx N_meas for random
  // Bernoulli matrices if normalized. Our entries are +/-1. A column has norm^2
  // = n_meas. So Lipschitz L ≈ n_meas. Step size <= 1/L. Let's be conservative.
  s->t_step = 0.5f / (float)n_meas;

  SAFE_CUBLAS(cublasCreate(&s->cublas));

  SAFE_CUDA(cudaMalloc(&s->d_x, s->npix * sizeof(float)));
  SAFE_CUDA(cudaMemset(s->d_x, 0, s->npix * sizeof(float)));

  // FISTA needs 'y' (auxiliary variable) and 'x_old'
  // We can reuse d_grad or alloc new. Let's act clean.
  // Reuse d_p for 'y' (momentum point)
  SAFE_CUDA(cudaMalloc(&s->d_p, s->npix * sizeof(float)));
  SAFE_CUDA(cudaMemset(s->d_p, 0, s->npix * sizeof(float)));

  // Reuse d_s for 'x_old'
  SAFE_CUDA(cudaMalloc(&s->d_s, s->npix * sizeof(float)));
  SAFE_CUDA(cudaMemset(s->d_s, 0, s->npix * sizeof(float)));

  SAFE_CUDA(cudaMalloc(&s->d_b, n_meas * sizeof(float)));
  SAFE_CUDA(cudaMemcpy(s->d_b, h_meas, n_meas * sizeof(float),
                       cudaMemcpyHostToDevice));

  // Gradient buffer
  SAFE_CUDA(cudaMalloc(&s->d_grad, s->npix * sizeof(float))); // Re-using d_grad

  // Residual buffer
  SAFE_CUDA(cudaMalloc(&s->d_r, n_meas * sizeof(float)));

  // FISTA t parameter
  s->gamma = 1.0f; // Reuse gamma for 't_k'

  s->d_masks = NULL;
  s->d_temp_meas = NULL;
  s->d_q = NULL;

  solver_init_common(s);
  return s;
fail:
  delete s;
  return NULL;
}

void solver_step(SolverState *s) {
  if (s->type == 1) {
    // ... (Old CGLS Logic - kept for compat)
    // --- CGLS STEP ---
    // q = A p
    ops_Ax(s, s->d_p, s->d_q);

    // alpha = gamma / ||q||^2
    float norm_q = 0.0f;
    cublasSdot(s->cublas, s->n_meas, s->d_q, 1, s->d_q, 1, &norm_q);
    if (norm_q < 1e-9f)
      return;
    float alpha = s->gamma / norm_q;

    // x = x + alpha * p
    cublasSaxpy(s->cublas, s->npix, &alpha, s->d_p, 1, s->d_x, 1);

    // r = r - alpha * q
    float minus_alpha = -alpha;
    cublasSaxpy(s->cublas, s->n_meas, &minus_alpha, s->d_q, 1, s->d_r, 1);

    // s = A^T r
    ops_ATy(s, s->d_r, s->d_s);

    // gamma_new = ||s||^2
    float gamma_new = 0.0f;
    cublasSdot(s->cublas, s->npix, s->d_s, 1, s->d_s, 1, &gamma_new);

    // beta = gamma_new / gamma
    float beta = gamma_new / s->gamma;

    // p = s + beta * p
    axpby(s->cublas, s->npix, 1.0f, s->d_s, beta, s->d_p);

    s->gamma = gamma_new;
  } else if (s->type == 10) {
    // --- FAST CGLS STEP ---
    // q = A p
    ops_Ax_fast(s, s->d_p, s->d_q);

    float norm_q = 0.0f;
    cublasSdot(s->cublas, s->n_meas, s->d_q, 1, s->d_q, 1, &norm_q);
    if (norm_q < 1e-9f)
      return;
    float alpha = s->gamma / norm_q;

    // x = x + alpha * p
    cublasSaxpy(s->cublas, s->npix, &alpha, s->d_p, 1, s->d_x, 1);

    // r = r - alpha * q
    float minus_alpha = -alpha;
    cublasSaxpy(s->cublas, s->n_meas, &minus_alpha, s->d_q, 1, s->d_r, 1);

    // s = A^T r
    ops_ATy_fast(s, s->d_r, s->d_s);

    float gamma_new = 0.0f;
    cublasSdot(s->cublas, s->npix, s->d_s, 1, s->d_s, 1, &gamma_new);

    float beta = gamma_new / s->gamma;

    // p = s + beta * p
    axpby(s->cublas, s->npix, 1.0f, s->d_s, beta, s->d_p);

    s->gamma = gamma_new;

  } else if (s->type == 2) {
    // ... (Old ISTA Logic)
    // --- ISTA STEP ---
    // 1. r = Ax
    ops_Ax(s, s->d_x, s->d_r);

    // 2. r = r - b
    float minus_one = -1.0f;
    cublasSaxpy(s->cublas, s->n_meas, &minus_one, s->d_b, 1, s->d_r, 1);

    // 3. grad = A^T r
    ops_ATy(s, s->d_r, s->d_grad);

    // 4. x = x - step * grad
    float minus_step = -s->t_step;
    cublasSaxpy(s->cublas, s->npix, &minus_step, s->d_grad, 1, s->d_x, 1);

    // 5. x = soft_threshold(x, step * lambda)
    int threads = 256;
    int blocks = (s->npix + threads - 1) / threads;
    soft_threshold_kernel<<<blocks, threads>>>(s->d_x, s->t_step * s->lambda,
                                               s->npix);
  } else if (s->type == 20) {
    // --- FAST FISTA STEP ---
    // State mapping:
    // d_x: x_k (Current estimate)
    // d_p: y_k (Momentum point - where we evaluate gradient)
    // d_s: x_{k-1} (Previous estimate)
    // gamma: t_k (Momentum scalar)

    // 1. Compute Gradient at y_k (d_p)
    // r = A * y_k
    ops_Ax_fast(s, s->d_p, s->d_r);

    // r = r - b
    float minus_one = -1.0f;
    cublasSaxpy(s->cublas, s->n_meas, &minus_one, s->d_b, 1, s->d_r, 1);

    // grad = A^T r
    ops_ATy_fast(s, s->d_r, s->d_grad);

    // 2. Update x_k -> x_{k+1}
    // Store x_k in temporary? No, we need x_k for momentum.
    // Save current x_k to d_s (which acts as x_old for NEXT step, but assumes
    // we have it) Actually, standard FISTA: x_{k}_new = soft(y_k - step * grad)
    // t_{k+1} = (1 + sqrt(1 + 4*t_k^2)) / 2
    // y_{k+1} = x_{k}_new + ((t_k - 1)/t_{k+1}) * (x_{k}_new - x_k)
    // x_k = x_{k}_new

    // Let's implement carefully:
    // d_s holds x_{old} (from start of step)
    CHECK_CUDA(cudaMemcpy(s->d_s, s->d_x, s->npix * sizeof(float),
                          cudaMemcpyDeviceToDevice));

    // x_new = y_k - step * grad
    // (copy y_k to d_x first)
    CHECK_CUDA(cudaMemcpy(s->d_x, s->d_p, s->npix * sizeof(float),
                          cudaMemcpyDeviceToDevice));
    float minus_step = -s->t_step;
    cublasSaxpy(s->cublas, s->npix, &minus_step, s->d_grad, 1, s->d_x, 1);

    // Soft threshold x_new
    int threads = 256;
    int blocks = (s->npix + threads - 1) / threads;
    soft_threshold_kernel<<<blocks, threads>>>(s->d_x, s->t_step * s->lambda,
                                               s->npix);

    // Update t
    float t_k = s->gamma;
    float t_new = (1.0f + sqrtf(1.0f + 4.0f * t_k * t_k)) / 2.0f;
    s->gamma = t_new;

    // Update y_{k+1} = x_new + momentum * (x_new - x_old)
    float momentum = (t_k - 1.0f) / t_new;

    // d_p = x_new
    CHECK_CUDA(cudaMemcpy(s->d_p, s->d_x, s->npix * sizeof(float),
                          cudaMemcpyDeviceToDevice));

    // d_p += momentum * (x_new - x_old)
    // calculation: (x_new - x_old) -> reuse d_grad or d_temp?
    // Let's use d_grad as temp since we are done with it for this step.

    // d_grad = x_new
    CHECK_CUDA(cudaMemcpy(s->d_grad, s->d_x, s->npix * sizeof(float),
                          cudaMemcpyDeviceToDevice));
    // d_grad = d_grad - x_old
    cublasSaxpy(s->cublas, s->npix, &minus_one, s->d_s, 1, s->d_grad, 1);

    // d_p = d_p + momentum * d_grad
    cublasSaxpy(s->cublas, s->npix, &momentum, s->d_grad, 1, s->d_p, 1);

    // Done. d_x has the current best estimate. d_p has the next probe point.
  }
}

void solver_destroy(SolverState *s) {
  if (!s)
    return;
  if (s->d_x)
    cudaFree(s->d_x);
  if (s->d_b)
    cudaFree(s->d_b);
  if (s->d_masks)
    cudaFree(s->d_masks);
  if (s->d_temp_meas)
    cudaFree(s->d_temp_meas);
  if (s->d_r)
    cudaFree(s->d_r);
  if (s->d_s)
    cudaFree(s->d_s);
  if (s->d_p)
    cudaFree(s->d_p);
  if (s->d_q)
    cudaFree(s->d_q);
  if (s->d_grad)
    cudaFree(s->d_grad);
  if (s->h_x_pinned)
    cudaFreeHost(s->h_x_pinned);
  if (s->h_target_pinned)
    cudaFreeHost(s->h_target_pinned);
  if (s->h_ranked)
    free(s->h_ranked);
  // curand_gen might not be initialized for fast solvers?
  // Check if created. fast solvers don't use it but they init to NULL or
  // structure is cleaned? In solver_create_ls_fast, we didn't init curand_gen.
  // It's garbage if not initialized? No, 'new SolverState' doesn't zero init!
  // I should have zero-inited in create functions.
  // But standard destructors handle it if I am careful.
  // Let's assume initialized to NULL or valid.
  // Wait, 'new SolverState' leaves fields uninitialized for POD structs!
  // I need to fix create functions to zero init or set to NULL.
  // But for now, let's just add destroy logic.
  if (s->curand_gen)
    curandDestroyGenerator(s->curand_gen);
  if (s->cublas)
    cublasDestroy(s->cublas);
  delete s;
}

void solver_get_image(SolverState *s, uint8_t *h_output) {
  if (!s || !s->d_x)
    return;
  float *h_temp = (float *)malloc(s->npix * sizeof(float));
  CHECK_CUDA(cudaMemcpy(h_temp, s->d_x, s->npix * sizeof(float),
                        cudaMemcpyDeviceToHost));
  for (int i = 0; i < s->npix; i++) {
    float v = h_temp[i];
    if (v < 0)
      v = 0;
    if (v > 255)
      v = 255;
    h_output[i] = (uint8_t)(v + 0.5f);
  }
  free(h_temp);
}

void solver_project_histogram(SolverState *s, const int *target_hist) {

  if (!s->h_x_pinned || !s->h_target_pinned || !s->h_ranked)
    return;

  // 1. Download x
  CHECK_CUDA(cudaMemcpy(s->h_x_pinned, s->d_x, s->npix * sizeof(float),
                        cudaMemcpyDeviceToHost));

  // 2. Sort x to find ranking of pixels
  for (int i = 0; i < s->npix; i++) {
    s->h_ranked[i].val = s->h_x_pinned[i];
    s->h_ranked[i].index = i;
  }
  std::sort(s->h_ranked, s->h_ranked + s->npix, comparePixels);

  // 3. Generate target values from histogram
  int ptr = 0;
  // target_hist is 256 bins
  for (int bin = 0; bin < 256; bin++) {
    int count = target_hist[bin];
    float val = (float)bin;
    for (int k = 0; k < count && ptr < s->npix; k++) {
      s->h_target_pinned[ptr++] = val;
    }
  }
  // Fill remaining if any
  while (ptr < s->npix) {
    if (ptr > 0)
      s->h_target_pinned[ptr] = s->h_target_pinned[ptr - 1];
    else
      s->h_target_pinned[ptr] = 0.0f;
    ptr++;
  }

  // 4. Assign target values to pixels based on rank
  for (int i = 0; i < s->npix; i++) {
    int original_idx = s->h_ranked[i].index;
    float target_val = s->h_target_pinned[i];
    s->h_x_pinned[original_idx] = target_val;
  }

  // 5. Upload x
  CHECK_CUDA(cudaMemcpy(s->d_x, s->h_x_pinned, s->npix * sizeof(float),
                        cudaMemcpyHostToDevice));
}

} /* extern C */
